{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1cafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data files (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love programming in Python\",\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"Word embeddings are useful in many NLP tasks\",\n",
    "    \"CBOW is a great way to train word vectors\"\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_corpus = []\n",
    "for sentence in corpus:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    tokenized_corpus.append(filtered_tokens)\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_corpus)\n",
    "total_words = len(tokenizer.word_index) + 1  # Plus one for padding\n",
    "\n",
    "# Create CBOW dataset\n",
    "def create_cbow_dataset(corpus, window_size):\n",
    "    context_words = []\n",
    "    target_words = []\n",
    "\n",
    "    for tokens in corpus:\n",
    "        for i, target in enumerate(tokens):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            context = [tokens[j] for j in range(start, end) if j != i]\n",
    "            target_words.append(target)\n",
    "            context_words.append(context)\n",
    "\n",
    "    return context_words, target_words\n",
    "\n",
    "# Set window size\n",
    "window_size = 2\n",
    "context, target = create_cbow_dataset(tokenized_corpus, window_size)\n",
    "\n",
    "# Convert context and target words to sequences\n",
    "context_sequences = tokenizer.texts_to_sequences([' '.join(c) for c in context])\n",
    "target_sequences = tokenizer.texts_to_sequences(target)\n",
    "\n",
    "# Pad the context sequences\n",
    "max_length = max(len(seq) for seq in context_sequences)  # Find the maximum length\n",
    "X = pad_sequences(context_sequences, maxlen=max_length, padding='post')\n",
    "y = np.array(target_sequences).flatten()\n",
    "\n",
    "# Convert target sequences to one-hot encoding\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
    "\n",
    "# Build the CBOW model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=total_words, output_dim=64, input_length=X.shape[1]),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# Example context to test\n",
    "test_context = \"the brown fox\".split()\n",
    "test_sequence = tokenizer.texts_to_sequences([' '.join(test_context)])\n",
    "\n",
    "# Pad the test sequence\n",
    "test_sequence = pad_sequences(test_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "# Make predictions\n",
    "predicted = model.predict(test_sequence)\n",
    "predicted_word_index = np.argmax(predicted, axis=1)\n",
    "\n",
    "# Display the predicted word\n",
    "predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
    "print(f\"Predicted Word for context '{' '.join(test_context)}': {predicted_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
